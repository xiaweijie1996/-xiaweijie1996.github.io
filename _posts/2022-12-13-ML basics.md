---
title: Why machine can learn? (Fundamental theory in statistic learning)
categories:
- Statistics
- Statistic Learning
feature_image: "https://scontent-ams2-1.xx.fbcdn.net/v/t39.30808-6/318727714_1298950054279522_1327222508011670093_n.jpg?_nc_cat=108&ccb=1-7&_nc_sid=730e14&_nc_ohc=TnxHVTXAD_IAX-HmZVu&_nc_ht=scontent-ams2-1.xx&oh=00_AfBSzcy1lv2fJGWPylw7pRDyxpG_NNwrFmcdUXnopbawrg&oe=6394F523"
---
<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>

**Introduction**\
Machine Learning models have been wildely used in many applications. But one question we barely consider is: Why can machine learn? In this article, I will discuss the fundamental reasons that allow machines to learn.


**Notations**
<table border="1">
    <tr>
        <td><font face="黑体" size=4><b>Symbol</b></font></td>
        <td><font face="黑体" size=4><b>Meaning</b></font></td>
    </tr>

    <tr>
        <td>$(X,Y)$</td>
        <td>Dataset</td>
    </tr>

    <tr>
        <td>$f:\mathcal{X} \rightarrow \mathcal{Y}$</td>
        <td>Real function</td>
    </tr>

    <tr>
        <td>$h$</td>
        <td>Hypothesis</td>
    </tr>

    <tr>
        <td>$x_{in}$</td>
        <td>Training dataset</td>
    </tr>

    <tr>
        <td>$x_{out}$</td>
        <td>Data set out of the training dataset $x_{out} = \mathcal{X}-x_{in}$</td>
    </tr>

    <tr>
        <td>$E_{in}(h)$</td>
        <td>Error of $h$ in $x_{in}$</td>
    </tr>

    <tr>
        <td>$E_{out}(h)$</td>
        <td>Error of $h$ in $x_{out}$</td>
    </tr>

    <tr>
        <td>$\mathcal{H}=(h_1,h_2,..,h_m)$</td>
        <td>Hypotheses</td>
    </tr>

    <tr>
        <td>$M$</td>
        <td>Number of Hypotheses</td>
    </tr>

    <tr>
        <td>$k$</td>
        <td>Break point</td>
    </tr>

</table>
<br />
<b>Definition</b><br />
Assuming we have dataset $(X,Y)$, and real function $f:\mathcal{X} \rightarrow \mathcal{Y}$, $h$ (hypothesis) represent the function learned by machine in the training dataset $x_{in}$, $E_{in}(h)$ represent the 
error of $h$ in the training dataset, $E_{out}(h)$ represent the error of $h$ out of the training dataset $x_{out} = \mathcal{X}-x_{in}$. Then, If a machine can learn, the following conditions must be satified:
<ul>
<li>$E_{in}(h) \approx 0 $, this means that $h$ performs good enough in training dataset. </li>
<li>$E_{in}(h) \approx E_{out}(h) $, this means that $h$ have generality can be expended to the whole dataset. </li>
<ul>
<br />
<b>Hoeffding's Inequality</b><br />
<img src="https://work.caltech.edu/images1/onebin.png" width = "300"  div align=center /><br />
Assuming a simple example, if we have a bin with green and red marbles inside, the probability of green marbles in a bin is $\mu$, now if we sample $N$ marbles from the bin, and the probability of green marbles in the 
samples is $\upsilon$, what the relationship between $\upsilon$ and $\mu$? Hoeffding's Inequality explains this relationship.

$$\mathbb{P}[|\upsilon-\mu|>\epsilon] \leq 2exp(-2\epsilon^2N)$$

Where $\epsilon$ is the difference between $\upsilon$ and $\mu$. That means if we have $N$ large enough, then the probability of $\epsilon$ being very big is small or the probability of $\epsilon$ being small is large. In other words, 
that means by studying the feature of samples, the machine can <b>probably approximately</b> learn the features of dataset $(X,Y)$ successfully. We can rewrite equation (1) by replacing $\upsilon$ and $\mu$ with $E$:

$$\mathbb{P}[|E_{in}(h)-E_{out}(h)|>\epsilon] \leq 2exp(-2\epsilon^2N)$$

Equation (1) and (2) is only for the cases when the number of hypothesis $|\mathcal{H}|=1$, but in reality, it is possible that we have more than 1 hypotheses $\mathcal{H}=(h_1,h_2,..,h_m)$, we use $M$ represents the number of hypotheseses $|\mathcal{H}|$, 
then Hoeffding's Inequality is:

$$\mathbb{P}[|E_{in}(h)-E_{out}(h)|>\epsilon] \leq 2Mexp(-2\epsilon^2N)$$

From (3), we know， if $M$ is finite, by increasing $N$, we can finally find a $h \in \mathcal{H}$, which makes $E_{in}(h) \approx E_{out}(h)$, and if $E_{in}(h) \approx 0$, then learning is possible.
<br />
<br />
<b>Growth Functions</b><br />
One apparent problem with (2) and (3) is if $M$ is infinite, then the right side of the equation will also be infinite, which might indicate the machine can not learn. But intuitively, this is obviously incorrect. For example, 
if we have 2 points located at $(0,0)$ and $(0,1)$ respectively and we want to separate these 2 points, then $M$ is infinite because any line passing between the points can perfectly separate them. However, this problem is simple and learnable for the machine.
In this section, we will solve this problem.
<br />
The method is using a finite quantity $m_{\mathcal{H}}$ to replace $M$, $m_{\mathcal{H}}$ represents the effective number of $\mathcal{H}$. In 2 ponint case above, $m_{\mathcal{H}}(2)=3$, because no matter how many lines can seperate 2 points,
there are only 3 effective results, which are 1) the line is to the left of 2 points 2) the line is in the middle of 2 points 3) the line is to the right of 2 points. For different training dataset $(x_1,x_2,..x_n)$, the growth is defined:

$$m_{\mathcal{H}}(N)=\max_{(x_1,x_2,..,x_n)\in \mathcal{X}}|\mathcal{H}(x_1,x_2,..,x_n)|$$

Then Hoeffding's Inequality is:

$$\mathbb{P}[|E_{in}(h)-E_{out}(h)|>\epsilon] \leq 2m_{\mathcal{H}}(N)exp(-2\epsilon^2N)$$
<br />
<b>Break point</b><br />
Until now, we have not solved the problem because the  $f_{N}=2m_{\mathcal{H}}(N)exp(-2\epsilon^2N)$ is a function of $N$, and we do not know its value. However, since the $f_{N,1}=2m_{\mathcal{H}}(N)$ is monotonically increasing and $f_{N,2}=exp(-2\epsilon^2N)$
monotonically decreasing, if we can find a $N_i$ satifies:
<ul>
<li> $f'_{N_i}=0$
<li> $f'_{N_{i+k}} \leq 0, k= 0,1,2,...$
<li> $\lim_{N \to +\infty} f_N=0$
<ul>
 We then define <b>break point</b> $k$, and $k=N_i$.





