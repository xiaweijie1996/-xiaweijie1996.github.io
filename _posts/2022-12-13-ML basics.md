---
title: Why machine can learn? (Fundamental theory in statistic learning)
categories:
- Statistics
- Statistic Learning
feature_image: "https://scontent-ams2-1.xx.fbcdn.net/v/t39.30808-6/318727714_1298950054279522_1327222508011670093_n.jpg?_nc_cat=108&ccb=1-7&_nc_sid=730e14&_nc_ohc=TnxHVTXAD_IAX-HmZVu&_nc_ht=scontent-ams2-1.xx&oh=00_AfBSzcy1lv2fJGWPylw7pRDyxpG_NNwrFmcdUXnopbawrg&oe=6394F523"
---
<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>

**Introduction**\
Machine Learning models have been wildely used in many applications. But, one question we barely consider is: Why can machine learn? In this article, I will discuss the fundamental reasons that allow machines to learn.


**Notations**
<table border="1">
    <tr>
        <td><font face="黑体" size=4><b>Symbol</b></font></td>
        <td><font face="黑体" size=4><b>Meaning</b></font></td>
    </tr>

    <tr>
        <td>$(X,Y)$</td>
        <td>Dataset</td>
    </tr>

    <tr>
        <td>$f:\mathcal{X} \rightarrow \mathcal{Y}$</td>
        <td>Real function</td>
    </tr>

    <tr>
        <td>$h$</td>
        <td>Hypothesis</td>
    </tr>

    <tr>
        <td>$x_{in}$</td>
        <td>Training dataset</td>
    </tr>

    <tr>
        <td>$x_{out}$</td>
        <td>Data set out of the training dataset $x_{out} = \mathcal{X}-x_{in}$</td>
    </tr>

    <tr>
        <td>$E_{in}(h)$</td>
        <td>Error of $h$ in the training dataset</td>
    </tr>

    <tr>
        <td>$E_{out}(h)$</td>
        <td>Error of $h$ in the training dataset</td>
    </tr>
</table>
<br />
Assuming we have dataset $(X,Y)$, and real function $f:\mathcal{X} \rightarrow \mathcal{Y}$, $h$ (hypothesis) represent the function learned by machine in the training dataset $x_{in}$, $E_{in}(h)$ represent the 
error of $h$ in the training dataset, $E_{out}(h)$ represent the error of $g$ out of the training dataset $x_{out} = \mathcal{X}-x_{in}$. Then, If a machine can learn, the following conditions must be satified:
<ul>
<li>$E_{in}(h) \approx 0 $, this means that $g$ performs good enough in training dataset. </li>
<li>$E_{in}(h) \approx E_{out}(h) $, this means that $h$ have generality can be expended to the whole dataset. </li>
<ul>

<b>Hoeffding's Inequality</b><br />
<div style="text-align: center; width: 500px; border: green solid 1px;">
<img alt="" src="https://work.caltech.edu/images1/onebin.png" style="margin: 0 auto;" />
</div>
Assuming a simple example, if we have a bin with green and red marbles inside, the probability of green marbles in a bin is $\mu$, now if we sample $N$ marbles from the bin, and the probability of green marbles in the 
sample is $\upsilon$, what the relationship between $\upsilon$ and $\mu$? Hoeffding's Inequality explains this relationship.

$$\mathbb{P}[|\upsilon-\mu|>\epsilon] \leq 2exp(-2\epsilon^2N)$$

Where \epsilon is the difference between $\upsilon$ and $\mu$. That means if we have $N$ large enough, then the probability of $\epsilon$ being very big is small or the probability of $\epsilon$ being small is big. *In other words, 
that means by studying the feature of samples, the machine can <b>probably approximately</b> learn the features of dataset $(X,Y)$ successfully*. We can rewrite equation (1) by replacing $\upsilon$ and $\mu$ with $E$:

$$\mathbb{P}[|E_{in}(h)-E_{out}(h)|>\epsilon] \leq 2exp(-2\epsilon^2N)$$

Equation (1) and (2) for only for the cases when the number of hypothesis $|\mathcal{H}|=1$， but in reality, it is possible that we have more then 1 hypothesises $\mathcal{H}=(h_1,h_2,..,h_m)$, we use $M$ represents the number of hypothesises $|\mathcal{H}|$, 
then the Hoeffding's Inequality is:

$$\mathbb{P}[|E_{in}(h)-E_{out}(h)|>\epsilon] \leq 2Mexp(-2\epsilon^2N)$$
