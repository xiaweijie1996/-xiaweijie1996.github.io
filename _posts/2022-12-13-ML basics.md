---
title: Why machine can learn? (Fundamental theory in statistic learning)
categories:
- Statistics
- Statistic Learning
feature_image: "https://scontent-ams2-1.xx.fbcdn.net/v/t39.30808-6/318727714_1298950054279522_1327222508011670093_n.jpg?_nc_cat=108&ccb=1-7&_nc_sid=730e14&_nc_ohc=TnxHVTXAD_IAX-HmZVu&_nc_ht=scontent-ams2-1.xx&oh=00_AfBSzcy1lv2fJGWPylw7pRDyxpG_NNwrFmcdUXnopbawrg&oe=6394F523"
---
<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>

**Introduction**\
Machine Learning models have been wildely used in many applications. But, one question we barely consider is: Why can machine learn? In this article, I will discuss the fundamental reasons that allow machines to learn.


**Notations**\
|Symbol|Meaning|
|  :----  | :----  |
| 单元格  | 单元格 |
| 单元格  | 单元格 |

Assuming we have dataset $(X,Y)$, and real function $f:\mathcal{X} \rightarrow \mathcal{Y}$, $h$ (hypothsis) represent the function learned by machine in the training dataset $x_{in}$, $E_{in}(h)$ represent the 
error of $g$ in the training dataset, $E_{out}(h)$ represent the error of $g$ out of the training dataset $x_{out} = \mathcal{X}-x_{in}$