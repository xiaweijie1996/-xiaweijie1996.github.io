---
title: Why machine can learn? (Fundamental theory in statistic learning)
categories:
- Statistics
- Statistic Learning
feature_image: "https://scontent-ams2-1.xx.fbcdn.net/v/t39.30808-6/318727714_1298950054279522_1327222508011670093_n.jpg?_nc_cat=108&ccb=1-7&_nc_sid=730e14&_nc_ohc=TnxHVTXAD_IAX-HmZVu&_nc_ht=scontent-ams2-1.xx&oh=00_AfBSzcy1lv2fJGWPylw7pRDyxpG_NNwrFmcdUXnopbawrg&oe=6394F523"
---
<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>

**Introduction**\
Machine Learning models have been wildely used in many applications. But, one question we barely consider is: Why can machine learn? In this article, I will discuss the fundamental reasons that allow machines to learn.


**Notations**
<table border="1">
    <tr>
        <td><font face="黑体" size=4><b>Symbol</b></font></td>
        <td><font face="黑体" size=4><b>Meaning</b></font></td>
    </tr>

    <tr>
        <td>$(X,Y)$</td>
        <td>Dataset</td>
    </tr>

    <tr>
        <td>$f:\mathcal{X} \rightarrow \mathcal{Y}$</td>
        <td>Real function</td>
    </tr>

    <tr>
        <td>$h$</td>
        <td>Hypothesis</td>
    </tr>

    <tr>
        <td>$x_{in}$</td>
        <td>Training dataset</td>
    </tr>

    <tr>
        <td>$x_{out}$</td>
        <td>Data set out of the training dataset $x_{out} = \mathcal{X}-x_{in}$</td>
    </tr>

    <tr>
        <td>$E_{in}(h)$</td>
        <td>Error of $h$ in the training dataset</td>
    </tr>

    <tr>
        <td>$E_{out}(h)$</td>
        <td>Error of $h$ in the training dataset</td>
    </tr>
</table>
<br />
Assuming we have dataset $(X,Y)$, and real function $f:\mathcal{X} \rightarrow \mathcal{Y}$, $h$ (hypothesis) represent the function learned by machine in the training dataset $x_{in}$, $E_{in}(h)$ represent the 
error of $h$ in the training dataset, $E_{out}(h)$ represent the error of $g$ out of the training dataset $x_{out} = \mathcal{X}-x_{in}$. Then, If a machine can learn, the following conditions must be satified:
<ul>
<li>$E_{in}(h) \approx 0 $, this means that $g$ performs good enough in training dataset. </li>
<li>$E_{in}(h) \approx E_{out}(h) $, this means that $h$ have generality can be expended to the whole dataset. </li>
<ul>

<b>Hoeffding's Inequality</b><br />
Assuming a simple example, if we have a bin with green and red marbles inside, the probability of green marbles in a bin is $p_r$, now if we sample %N% marbles from the bin, and the probability of green marbles in the 
sample is $p_s$, what the relationship between $p_s$ and $p_r$? Hoeffding's Inequality explains this relationship.
W
$$\mathbb{P}[|p_s-p_r|>\epsilon] \leq 2exp(-2\epsilon^2N)$$

Where \epsilon is the difference between $p_s$ and $p_r$. That means if we have $N$ large enough, then the probability of $\epsilon$ being very big is small or the probability of $\epsilon$ being small is big.