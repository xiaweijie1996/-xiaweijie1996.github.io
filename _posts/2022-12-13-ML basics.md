---
title: Why machine can learn? (Fundamental theory in statistic learning)
categories:
- Statistics
- Statistical Learning
feature_image: "https://i.postimg.cc/mg4Xrb3s/wallhaven-28p666-2560x600.png"
---
<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>

**Introduction**\
Machine Learning models have been widely used in many applications. But one question we barely consider is: Why can machines learn? In this article, I will discuss the fundamental reasons that allow machines to learn.

**Notations**
<table border="1">
    <tr>
        <td><font face="黑体" size=4><b>Symbol</b></font></td>
        <td><font face="黑体" size=4><b>Meaning</b></font></td>
    </tr>

    <tr>
        <td>$(X,Y)$</td>
        <td>Dataset</td>
    </tr>

    <tr>
        <td>$\Omega$</td>
        <td>Penalty term</td>
    </tr>

    <tr>
        <td>$h$</td>
        <td>Hypothesis</td>
    </tr>

    <tr>
        <td>$x_{in}$</td>
        <td>Training dataset</td>
    </tr>

    <tr>
        <td>$x_{out}$</td>
        <td>Data set out of the training dataset $x_{out} = \mathcal{X}-x_{in}$</td>
    </tr>

    <tr>
        <td>$E_{in}(h)$</td>
        <td>Error of $h$ in $x_{in}$</td>
    </tr>

    <tr>
        <td>$E_{out}(h)$</td>
        <td>Error of $h$ in $x_{out}$</td>
    </tr>

    <tr>
        <td>$\mathcal{H}=(h_1,h_2,..,h_m)$</td>
        <td>Hypotheses</td>
    </tr>

    <tr>
        <td>$M$</td>
        <td>Number of Hypotheses</td>
    </tr>

    <tr>
        <td>$k$</td>
        <td>Break point</td>
    </tr>

</table>
<br />
<b>Definition</b><br />
Assuming we have dataset $(X,Y)$, and real function $f:\mathcal{X} \rightarrow \mathcal{Y}$, $h$ (hypothesis) represent the function learned by machine in the training dataset $x_{in}$, $E_{in}(h)$ represent the 
error of $h$ in the training dataset, $E_{out}(h)$ represent the error of $h$ out of the training dataset $x_{out} = \mathcal{X}-x_{in}$. Then, If a machine can learn, the following conditions must be satified:
<ul>
   <li>$E_{in}(h) \approx 0 $, this means that $h$ performs good enough in training dataset. </li>
   <li>$E_{in}(h) \approx E_{out}(h) $, this means that $h$ have generality can be expended to the whole dataset. </li>
<ul>
<br />
<b>Hoeffding's Inequality</b><br />
<img src="https://work.caltech.edu/images1/onebin.png" width = "300"  div align=center /><br />
Assuming a simple example, if we have a bin with green and red marbles inside, the probability of green marbles in a bin is $\mu$. Now, if we sample $N$ marbles from the bin, and the probability of green marbles in the 
sample is $\upsilon$, what is he relationship between $\upsilon$ and $\mu$? Hoeffding's Inequality explains this relationship.

$$\mathbb{P}[|\upsilon-\mu|>\epsilon] \leq 2exp(-2\epsilon^2N)$$

Where $\epsilon$ is the difference between $\upsilon$ and $\mu$, that means if we have $N$ large enough, then the probability of $\epsilon$ being very big is small or the probability of $\epsilon$ being small is large. In other words, 
by studying the feature of samples, the machine can <b>probably approximately</b> learn the features of yhe dataset $(X,Y)$ successfully. We can rewrite equation (1) by replacing $\upsilon$ and $\mu$ with $E$:

$$\mathbb{P}[|E_{in}(h)-E_{out}(h)|>\epsilon] \leq 2exp(-2\epsilon^2N)$$

Equation (1) and (2) is only for the cases when the number of hypothesis $|\mathcal{H}|=1$, but in reality, it is possible that we have more than 1 hypothesis $\mathcal{H}=(h_1,h_2,..,h_m)$, we use $M$ represents the number of hypothesises $|\mathcal{H}|$, 
then Hoeffding's Inequality is:

$$\mathbb{P}[|E_{in}(h)-E_{out}(h)|>\epsilon] \leq 2Mexp(-2\epsilon^2N)$$

From (3), we know if $M$ is finite, by increasing $N$, we can finally find a $h \in \mathcal{H}$, which makes $E_{in}(h) \approx E_{out}(h)$, and if $E_{in}(h) \approx 0$, then learning is possible.
<br />
<br />
<b>Growth Functions</b><br />
One apparent problem with (2) and (3) is if $M$ is infinite, then the right side of the equation will also be infinite, which might indicate the machine can not learn. But intuitively, this is obviously incorrect. For example, 
For example, suppose we have 2 points located at $(0,0)$ and $(0,1)$ ,respectively, and we want to separate these 2 points, In that case, $M$ is infinite because any line passing between the points can perfectly separate them. However, this problem is simple and learnable for the machine.
In this section, we will solve this problem.
<br />
The method is to use a finite quantity $m_{\mathcal{H}}$ to replace $M$, $m_{\mathcal{H}}$ represents the effective number of $\mathcal{H}$. In 2 point case above, $m_{\mathcal{H}}(2)=3$, because no matter how many lines can separate(classify) 2 points,
there are only 3 effective results, which are 1) the line is to the left of 2 points 2) the line is in the middle of 2 points 3) the line is to the right of 2 points. For different training dataset $(x_1,x_2,..x_n)$, the growth is defined:

$$m_{\mathcal{H}}(N)=\max_{(x_1,x_2,..,x_n)\in \mathcal{X}}|\mathcal{H}(x_1,x_2,..,x_n)|$$

Then Hoeffding's Inequality is:

$$\mathbb{P}[|E_{in}(h)-E_{out}(h)|>\epsilon] \leq 2m_{\mathcal{H}}(N)exp(-2\epsilon^2N)$$

<br />
<b>Break point</b><br />
Until now, we have not solved the problem because the  $f_{N}=2m_{\mathcal{H}}(N)exp(-2\epsilon^2N)$ is a function of $N$, and we do not know its value. However, since the $f_{N,1}=2m_{\mathcal{H}}(N)$ is monotonically increasing and $f_{N,2}=exp(-2\epsilon^2N)$
monotonically decreasing, if we can find a $N_i$ satisfies:

<ul>
    <li>$f'_{N_{i-1}} > 0$</li>
    <li>$f'_{N_{i+j}} \leq 0, j= 0,1,2,...$</li>
    <li>$\lim_{N \to +\infty} f_N=0$</li>
<ul>

We then define <b>break point</b> $k$, and $k=N_i$.<br />
In fact, by statistical derivation, we can get <b>Vapnik-Chervonenkis (VC) Bound</b> function (6) and <b>VC dimension</b> $d_{vc}(\mathcal{H})=k-1$:

$$\mathbb{P}[\exists h \in \mathcal{H} s.t. |E_{in}(h)-E_{out}(h)|>\epsilon] \leq 4(2N)^{d_{vc}}exp(-\frac{1}{8}\epsilon^2N)$$

$$m_{\mathcal{H}}(2N) \leq N^{d_{vc}}$$

<br />
<b>Penalty for Model Complexity</b><br />
Set:<br />

$$\delta = 4(2N)^{d_{vc}}exp(-\frac{1}{8}\epsilon^2N)$$

Then:<br />

$$\sqrt{\frac{8}{N}ln(\frac{4(2N)^{d_{vc}}}{\delta})}=\epsilon$$

by replace $\epsilon$ with (8):

$$|E_{in}(h)-E_{out}(h)|> \sqrt{\frac{8}{N}ln(\frac{4(2N)^{d_{vc}}}{\delta})} $$

Then, we get:

$$E_{in}(h)-\sqrt{\frac{8}{N}ln(\frac{4(2N)^{d_{vc}}}{\delta})}  \leq E_{out}(h) \leq E_{in}(h)+ \underbrace{\sqrt{\frac{8}{N}ln(\frac{4(2N)^{d_{vc}}}{\delta})}}_{\Omega(\delta,N,\mathbb{H})}$$

The $\Omega(\delta,N,\mathbb{H})$ is essentially represents the penalty from model complexity:

<ul>
    <li> $d_{vc} \uparrow: E_{in} \downarrow, \Omega\uparrow$</li>
    <li> $d_{vc} \downarrow: E_{in} \uparrow, \Omega\downarrow$</li>
<ul>

<img src="https://encrypted-tbn1.gstatic.com/images?q=tbn:ANd9GcRNNaKrr2Q5v9FbS5zWaLwnttwwOVpuomj1xOGCHkjigJuva2Gr" width = "300"  div align=center /><br />




